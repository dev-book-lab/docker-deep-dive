# 07. High Availability - ê³ ê°€ìš©ì„±

## ğŸ¯ ì´ ì±•í„°ì—ì„œ ë°°ìš¸ ê²ƒ

- **ê³ ê°€ìš©ì„±(HA)** ì•„í‚¤í…ì²˜ ì„¤ê³„
- **ì¥ì•  ë³µêµ¬** ë©”ì»¤ë‹ˆì¦˜
- **ë°ì´í„° ë³µì œ**ì™€ ë°±ì—…
- **ë¶„ì‚° ì‹œìŠ¤í…œ** íŒ¨í„´

## ğŸ“Œ ì™œ ì¤‘ìš”í•œê°€?

**"ê³ ê°€ìš©ì„±ì€ ì‹œìŠ¤í…œì´ ì¥ì•  ìƒí™©ì—ì„œë„ ì§€ì†ì ìœ¼ë¡œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ëŠ¥ë ¥ì…ë‹ˆë‹¤."**

```
ë‹¨ì¼ ì¥ì• ì  vs ê³ ê°€ìš©ì„±:

ë‹¨ì¼ ì¥ì• ì  (SPOF - Single Point of Failure):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Single Server                       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚ â”‚   App   â”‚                         â”‚
â”‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚
â”‚      â”‚                              â”‚
â”‚ â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”                         â”‚
â”‚ â”‚   DB    â”‚                         â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âŒ ì„œë²„ ì¥ì•  = ì„œë¹„ìŠ¤ ì¤‘ë‹¨
âŒ ë³µêµ¬ ì‹œê°„ í•„ìš”
âŒ ë°ì´í„° ì†ì‹¤ ìœ„í—˜
âŒ ì‚¬ìš©ì ì˜í–¥ í¼

ê³ ê°€ìš©ì„± ì•„í‚¤í…ì²˜:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Load Balancer (HA)                â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚ â”‚  LB 1  â”‚ â”‚  LB 2   â”‚            â”‚
â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚           â”‚
  â”Œâ”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
  â”‚       â”‚         â”‚       â”‚
â”Œâ”€â–¼â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â” â”Œâ”€â”€â”€â–¼â”€â”
â”‚App 1â”‚ â”‚App 2â”‚ â”‚App 3â”‚ â”‚App 4â”‚
â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜
   â”‚       â”‚       â”‚       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚       â”‚
      â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
      â”‚ Database Clusterâ”‚
      â”‚ â”Œâ”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”  â”‚
      â”‚ â”‚ M  â”‚â†â†’â”‚ R  â”‚  â”‚
      â”‚ â””â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”˜  â”‚
      â”‚ Primary Replica â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… ë…¸ë“œ ì¥ì•  ì‹œ ìë™ ë³µêµ¬
âœ… ë¬´ì¤‘ë‹¨ ì„œë¹„ìŠ¤
âœ… ë°ì´í„° ë³µì œ/ë°±ì—…
âœ… ì‚¬ìš©ì ì˜í–¥ ìµœì†Œí™”

ê³ ê°€ìš©ì„±ì˜ í•µì‹¬ ê°€ì¹˜:

1. ê°€ìš©ì„± ëª©í‘œ (SLA):
   - 99.9% (Three Nines): ì—° 8.76ì‹œê°„ ë‹¤ìš´íƒ€ì„
   - 99.99% (Four Nines): ì—° 52.6ë¶„ ë‹¤ìš´íƒ€ì„
   - 99.999% (Five Nines): ì—° 5.26ë¶„ ë‹¤ìš´íƒ€ì„

2. ì¥ì•  í—ˆìš© (Fault Tolerance):
   - ë…¸ë“œ ì¥ì• 
   - ë„¤íŠ¸ì›Œí¬ ì¥ì• 
   - ë””ìŠ¤í¬ ì¥ì• 
   - ë°ì´í„°ì„¼í„° ì¥ì• 

3. ìë™ ë³µêµ¬ (Self-Healing):
   - ì¥ì•  ê°ì§€
   - ìë™ ì¬ì‹œì‘
   - íŠ¸ë˜í”½ ì¬ë¶„ë°°
   - ë°ì´í„° ë³µêµ¬

4. ë¬´ì¤‘ë‹¨ ìš´ì˜:
   - ë¡¤ë§ ì—…ë°ì´íŠ¸
   - ìœ ì§€ë³´ìˆ˜
   - í™•ì¥/ì¶•ì†Œ

ì‹¤ë¬´ ì‹œë‚˜ë¦¬ì˜¤:

E-Commerce í”Œë«í¼:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Region 1 (Primary)                  â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Manager Nodes (3)             â”‚   â”‚
â”‚ â”‚ â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”          â”‚   â”‚
â”‚ â”‚ â”‚ M1 â”‚ â”‚ M2 â”‚ â”‚ M3 â”‚          â”‚   â”‚
â”‚ â”‚ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜          â”‚   â”‚
â”‚ â”‚ (Raft Consensus)              â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Worker Nodes (10)             â”‚   â”‚
â”‚ â”‚ - Web (3 replicas)            â”‚   â”‚
â”‚ â”‚ - API (5 replicas)            â”‚   â”‚
â”‚ â”‚ - Workers (5 replicas)        â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Database Cluster              â”‚   â”‚
â”‚ â”‚ - Primary (1)                 â”‚   â”‚
â”‚ â”‚ - Replicas (2)                â”‚   â”‚
â”‚ â”‚ - Auto Failover               â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†• Replication
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Region 2 (DR Standby)               â”‚
â”‚ - Passive Replica                   â”‚
â”‚ - 15ë¶„ RPO                          â”‚
â”‚ - 30ë¶„ RTO                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤:
- Worker 1ëŒ€ ì¥ì•  â†’ ìë™ ë³µêµ¬ (30ì´ˆ)
- Manager 1ëŒ€ ì¥ì•  â†’ ì¿¼ëŸ¼ ìœ ì§€, ì •ìƒ ìš´ì˜
- Database Primary ì¥ì•  â†’ Replica ìŠ¹ê²© (1ë¶„)
- ì „ì²´ Region ì¥ì•  â†’ DR ì „í™˜ (30ë¶„)
```

**ì‹¤ë¬´ ì˜í–¥:**
- SLA ë‹¬ì„±
- ê³ ê° ì‹ ë¢°
- ë§¤ì¶œ ì†ì‹¤ ë°©ì§€
- ë¸Œëœë“œ ê°€ì¹˜

---

## ğŸ”¬ Deep Dive

### 1. Manager ë…¸ë“œ ê³ ê°€ìš©ì„±

#### Raft Consensus

```
Raft í•©ì˜ ì•Œê³ ë¦¬ì¦˜:

3 Manager Cluster:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Manager 1â”‚  â”‚Manager 2â”‚  â”‚Manager 3â”‚
â”‚(Leader) â”‚  â”‚         â”‚  â”‚         â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚            â”‚            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              Heartbeat

ì¥ì•  ì‹œë‚˜ë¦¬ì˜¤:

1. Manager 1 ì¥ì•  (Leader):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Manager 1â”‚  â”‚Manager 2â”‚  â”‚Manager 3â”‚
â”‚   âŒ    â”‚  â”‚         â”‚  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                  â”‚            â”‚
              Election (10ì´ˆ)
                  â”‚            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Manager 1â”‚  â”‚Manager 2â”‚  â”‚Manager 3â”‚
â”‚   âŒ    â”‚  â”‚(Leader) â”‚  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
âœ… ìƒˆ ë¦¬ë” ì„ ì¶œ, ì •ìƒ ìš´ì˜

2. Manager 2ëŒ€ ë™ì‹œ ì¥ì• :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Manager 1â”‚  â”‚Manager 2â”‚  â”‚Manager 3â”‚
â”‚   âŒ    â”‚  â”‚   âŒ    â”‚  â”‚         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                               â”‚
                          ì¿¼ëŸ¼ ìƒì‹¤
âŒ í´ëŸ¬ìŠ¤í„° ì •ì§€
âš ï¸ Read-Only ëª¨ë“œ

ê¶Œì¥ êµ¬ì„±:
- 1 Manager: ì¥ì•  í—ˆìš© 0
- 3 Managers: ì¥ì•  í—ˆìš© 1 (ê¶Œì¥)
- 5 Managers: ì¥ì•  í—ˆìš© 2 (ëŒ€ê·œëª¨)
- 7 Managers: ì¥ì•  í—ˆìš© 3 (ì´ˆëŒ€ê·œëª¨)
```

#### Manager ê³ ê°€ìš©ì„± êµ¬ì„±

```bash
# 3 Manager í´ëŸ¬ìŠ¤í„° êµ¬ì„±

# Node 1 (Manager 1)
docker swarm init --advertise-addr 192.168.1.101

# Manager Join Token
MANAGER_TOKEN=$(docker swarm join-token manager -q)

# Node 2 (Manager 2)
docker swarm join \
  --token $MANAGER_TOKEN \
  --advertise-addr 192.168.1.102 \
  192.168.1.101:2377

# Node 3 (Manager 3)
docker swarm join \
  --token $MANAGER_TOKEN \
  --advertise-addr 192.168.1.103 \
  192.168.1.101:2377

# í™•ì¸
docker node ls
# ID         HOSTNAME   STATUS  AVAILABILITY  MANAGER STATUS
# abc... *   manager1   Ready   Active        Leader
# def...     manager2   Ready   Active        Reachable
# ghi...     manager3   Ready   Active        Reachable

# Manager ì „ìš© (ì›Œí¬ë¡œë“œ ì œê±°)
docker node update --availability drain manager1
docker node update --availability drain manager2
docker node update --availability drain manager3
```

#### Manager ì¥ì•  ë³µêµ¬ ì‹œë®¬ë ˆì´ì…˜

```bash
# ë¦¬ë” ë…¸ë“œ ê°•ì œ ì¤‘ì§€
docker node inspect manager1 --format '{{.ManagerStatus.Leader}}'
# true

# manager1 ì¤‘ì§€ (ì‹œë®¬ë ˆì´ì…˜)
# SSH to manager1
sudo systemctl stop docker

# ë‹¤ë¥¸ Managerì—ì„œ í™•ì¸
docker node ls
# manager1: Down
# manager2: Leader (ìƒˆë¡œ ì„ ì¶œë¨!)
# manager3: Reachable

# ì„œë¹„ìŠ¤ëŠ” ì •ìƒ ìš´ì˜
docker service ls
# âœ… ëª¨ë“  ì„œë¹„ìŠ¤ ì •ìƒ

# manager1 ë³µêµ¬
# SSH to manager1
sudo systemctl start docker

# í™•ì¸
docker node ls
# manager1: Ready, Reachable (ë³µêµ¬ë¨)
# manager2: Ready, Leader
# manager3: Ready, Reachable
```

---

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ê³ ê°€ìš©ì„±

#### ë ˆí”Œë¦¬ì¹´ ë¶„ì‚°

```bash
# Swarm ì´ˆê¸°í™” (3 Worker)
docker swarm init

# ë…¸ë“œ ë ˆì´ë¸” (ê°€ìš© ì˜ì—­)
docker node update --label-add zone=zone-a worker1
docker node update --label-add zone=zone-b worker2
docker node update --label-add zone=zone-c worker3

# Zoneë³„ ë¶„ì‚° ë°°ì¹˜
docker service create \
  --name web \
  --replicas 9 \
  --placement-pref spread=node.labels.zone \
  --publish 8080:80 \
  nginx:alpine

# í™•ì¸
docker service ps web --format '{{.Name}}\t{{.Node}}'
# web.1  worker1 (zone-a)
# web.2  worker2 (zone-b)
# web.3  worker3 (zone-c)
# web.4  worker1 (zone-a)
# web.5  worker2 (zone-b)
# web.6  worker3 (zone-c)
# web.7  worker1 (zone-a)
# web.8  worker2 (zone-b)
# web.9  worker3 (zone-c)
# âœ… ê· ë“± ë¶„ì‚°!

# Worker 1 ì¥ì•  ì‹œ
# â†’ web.1, web.4, web.7 ìë™ ì¬ë°°ì¹˜
# â†’ worker2, worker3ë¡œ ì´ë™
```

#### ìë™ ë³µêµ¬

```bash
# ì„œë¹„ìŠ¤ ìƒì„±
docker service create \
  --name app \
  --replicas 5 \
  --restart-condition on-failure \
  --restart-max-attempts 3 \
  --restart-delay 10s \
  myapp:latest

# ì»¨í…Œì´ë„ˆ ê°•ì œ ì¢…ë£Œ (ì¥ì•  ì‹œë®¬ë ˆì´ì…˜)
CONTAINER_ID=$(docker ps -q -f name=app | head -1)
docker kill $CONTAINER_ID

# ìë™ ì¬ì‹œì‘ í™•ì¸
docker service ps app
# ID      NAME      IMAGE         DESIRED  CURRENT
# abc...  app.1     myapp:latest  Running  Running (10s ago)
# def...   \_ app.1 myapp:latest  Shutdown Failed (killed)
# âœ… ìë™ ì¬ì‹œì‘ë¨!
```

#### Anti-Affinity (ë¶„ì‚° ë°°ì¹˜)

```bash
# ê°™ì€ ë…¸ë“œì— ì—¬ëŸ¬ ë ˆí”Œë¦¬ì¹´ ë°©ì§€
docker service create \
  --name db \
  --replicas 3 \
  --constraint 'node.labels.type==database' \
  --placement-max-replicas-per-node 1 \
  postgres:14-alpine

# í™•ì¸
docker service ps db
# db.1  node1
# db.2  node2
# db.3  node3
# âœ… ê° ë…¸ë“œì— 1ê°œì”©ë§Œ!

# node1 ì¥ì•  ì‹œ
# â†’ db.1ë§Œ ì˜í–¥
# â†’ db.2, db.3ì€ ì •ìƒ ìš´ì˜
```

---

### 3. ë°ì´í„°ë² ì´ìŠ¤ ê³ ê°€ìš©ì„±

#### Primary-Replica êµ¬ì¡°

```yaml
# stack.yml
version: '3.8'

services:
  # Primary Database
  db-primary:
    image: postgres:14-alpine
    environment:
      POSTGRES_PASSWORD: secret
      POSTGRES_DB: myapp
    volumes:
      - db-primary-data:/var/lib/postgresql/data
    networks:
      - db-network
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.db-role==primary

  # Replica 1
  db-replica1:
    image: postgres:14-alpine
    environment:
      POSTGRES_PASSWORD: secret
      PRIMARY_HOST: db-primary
    volumes:
      - db-replica1-data:/var/lib/postgresql/data
    networks:
      - db-network
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.db-role==replica

  # Replica 2
  db-replica2:
    image: postgres:14-alpine
    environment:
      POSTGRES_PASSWORD: secret
      PRIMARY_HOST: db-primary
    volumes:
      - db-replica2-data:/var/lib/postgresql/data
    networks:
      - db-network
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.db-role==replica

  # PgPool (Connection Pooling + Load Balancing)
  pgpool:
    image: pgpool/pgpool:latest
    environment:
      POSTGRES_PASSWORD: secret
      PGPOOL_BACKEND_NODES: "0:db-primary:5432,1:db-replica1:5432,2:db-replica2:5432"
      PGPOOL_ENABLE_LOAD_BALANCING: "yes"
    networks:
      - db-network
    ports:
      - "5432:5432"
    deploy:
      replicas: 2

networks:
  db-network:
    driver: overlay

volumes:
  db-primary-data:
  db-replica1-data:
  db-replica2-data:
```

#### ì¥ì•  ë³µêµ¬ ì‹œë‚˜ë¦¬ì˜¤

```bash
# Primary ì¥ì•  ì‹œ:
# 1. Replica ìŠ¹ê²© (Manual or Auto)
docker service update \
  --label-add role=primary \
  db-replica1

# 2. ë‹¤ë¥¸ Replicaë¥¼ ìƒˆ Primaryì— ì—°ê²°
docker service update \
  --env-add PRIMARY_HOST=db-replica1 \
  db-replica2

# 3. ìƒˆ Replica ì¶”ê°€
docker service create \
  --name db-replica3 \
  --env PRIMARY_HOST=db-replica1 \
  postgres:14-alpine
```

---

### 4. ë°±ì—…ê³¼ ë³µêµ¬

#### ìë™ ë°±ì—…

```yaml
# backup.yml
version: '3.8'

services:
  # Backup Service
  backup:
    image: postgres:14-alpine
    networks:
      - db-network
    volumes:
      - db-primary-data:/var/lib/postgresql/data:ro
      - ./backups:/backups
    environment:
      POSTGRES_PASSWORD: secret
    entrypoint: >
      sh -c "
        while true; do
          echo 'Starting backup at $(date)'
          
          # Full backup
          BACKUP_FILE=/backups/full-backup-$(date +%Y%m%d-%H%M%S).sql
          PGPASSWORD=secret pg_dumpall -h db-primary -U postgres > \$$BACKUP_FILE
          
          # Compress
          gzip \$$BACKUP_FILE
          
          # Retention (30 days)
          find /backups -name '*.sql.gz' -mtime +30 -delete
          
          echo 'Backup completed'
          
          # Daily at 2 AM
          sleep 86400
        done
      "
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.backup==true

networks:
  db-network:
    external: true

volumes:
  db-primary-data:
    external: true
```

#### ì¬í•´ ë³µêµ¬ (DR)

```yaml
# dr-sync.yml
version: '3.8'

services:
  # DR Sync (Continuous Replication)
  dr-sync:
    image: alpine
    volumes:
      - db-primary-data:/source:ro
      - nfs-dr:/dr-site
    entrypoint: >
      sh -c "
        apk add --no-cache rsync
        
        while true; do
          echo 'Syncing to DR site...'
          
          rsync -avz --delete \
            /source/ \
            /dr-site/postgresql/
          
          echo 'Sync completed at $(date)'
          
          # Every 15 minutes
          sleep 900
        done
      "
    deploy:
      replicas: 1

volumes:
  db-primary-data:
    external: true
  
  nfs-dr:
    driver: local
    driver_opts:
      type: nfs
      o: addr=dr-site.example.com,rw
      device: ":/dr-backup"
```

---

## ğŸ’» ì‹¤ìŠµ

### ì‹¤ìŠµ 1: Manager ê³ ê°€ìš©ì„±

```bash
# Swarm ì´ˆê¸°í™”
docker swarm init

# ìì‹ ì„ 3ê°œë¡œ í™•ì¥ (ì‹œë®¬ë ˆì´ì…˜)
# ì‹¤ì œë¡œëŠ” 3ê°œ ë…¸ë“œ í•„ìš”

# í˜„ì¬ Manager ìƒíƒœ
docker node ls

# ë¦¬ë” í™•ì¸
docker node inspect $(docker node ls -q) \
  --format '{{.ManagerStatus.Leader}}'
# true

# Auto-lock í™œì„±í™” (ë³´ì•ˆ)
docker swarm update --autolock=true
# í‚¤ ì €ì¥ í•„ìˆ˜!

# Manager ì¬ì‹œì‘ ì‹œë®¬ë ˆì´ì…˜
sudo systemctl restart docker

# Unlock í•„ìš”
docker swarm unlock
# í‚¤ ì…ë ¥

# ì •ë¦¬
docker swarm leave --force
```

---

### ì‹¤ìŠµ 2: ì• í”Œë¦¬ì¼€ì´ì…˜ ìë™ ë³µêµ¬

```bash
# Swarm ì´ˆê¸°í™”
docker swarm init

# ì„œë¹„ìŠ¤ ìƒì„± (ìë™ ë³µêµ¬ ì„¤ì •)
docker service create \
  --name web \
  --replicas 5 \
  --publish 8080:80 \
  --restart-condition on-failure \
  --restart-max-attempts 5 \
  --restart-delay 10s \
  --health-cmd "wget -q --spider http://localhost || exit 1" \
  --health-interval 10s \
  --health-timeout 5s \
  --health-retries 3 \
  nginx:alpine

# ì •ìƒ ë™ì‘ í™•ì¸
docker service ps web

# ì¥ì•  ì‹œë®¬ë ˆì´ì…˜ (ì»¨í…Œì´ë„ˆ ê°•ì œ ì¢…ë£Œ)
CONTAINER_ID=$(docker ps -q -f name=web | head -1)
docker kill $CONTAINER_ID

# ìë™ ë³µêµ¬ í™•ì¸
watch -n 1 'docker service ps web | head -10'
# ìƒˆ íƒœìŠ¤í¬ ìë™ ìƒì„±ë¨!

# ì—¬ëŸ¬ ë²ˆ ì¥ì•  ë°œìƒ (5íšŒ ì´ˆê³¼)
for i in {1..6}; do
  CONTAINER_ID=$(docker ps -q -f name=web | head -1)
  docker kill $CONTAINER_ID
  sleep 5
done

# í™•ì¸
docker service ps web
# 5íšŒ ì¬ì‹œì‘ í›„ ì¤‘ì§€ë¨

# ì •ë¦¬
docker service rm web
docker swarm leave --force
```

---

### ì‹¤ìŠµ 3: ë¶„ì‚° ë°°ì¹˜

```bash
# Swarm ì´ˆê¸°í™”
docker swarm init

# ë…¸ë“œì— ë ˆì´ë¸” (Zone ì‹œë®¬ë ˆì´ì…˜)
NODE_ID=$(docker node ls -q)
docker node update --label-add zone=zone-a $NODE_ID

# ì„œë¹„ìŠ¤ ìƒì„± (Zoneë³„ ë¶„ì‚°)
docker service create \
  --name app \
  --replicas 6 \
  --placement-pref spread=node.labels.zone \
  nginx:alpine

# í™•ì¸
docker service ps app

# Anti-Affinity (ë…¸ë“œë‹¹ ìµœëŒ€ 2ê°œ)
docker service create \
  --name db \
  --replicas 4 \
  --placement-max-replicas-per-node 2 \
  postgres:14-alpine

# í™•ì¸
docker service ps db

# ì •ë¦¬
docker service rm app db
docker swarm leave --force
```

---

## ğŸ”¥ ì‹¤ì „ ì‹œë‚˜ë¦¬ì˜¤

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ì „í•œ HA ìŠ¤íƒ

```yaml
# ha-stack.yml
version: '3.8'

services:
  # Nginx (Load Balancer)
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    networks:
      - frontend
    configs:
      - source: nginx-config
        target: /etc/nginx/nginx.conf
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1
        preferences:
          - spread: node.labels.zone
      update_config:
        parallelism: 1
        delay: 30s
        order: start-first
      restart_policy:
        condition: on-failure
        max_attempts: 3

  # Application
  app:
    image: myapp:latest
    networks:
      - frontend
      - backend
    environment:
      DB_HOST: pgpool
      REDIS_HOST: redis-sentinel
    deploy:
      replicas: 10
      placement:
        preferences:
          - spread: node.labels.zone
      update_config:
        parallelism: 2
        delay: 30s
        failure_action: rollback
        order: start-first
      restart_policy:
        condition: on-failure
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # PostgreSQL Primary
  postgres-primary:
    image: postgres:14-alpine
    networks:
      - backend
    environment:
      POSTGRES_PASSWORD: secret
      POSTGRES_DB: myapp
    volumes:
      - postgres-primary:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.db-role==primary
      restart_policy:
        condition: on-failure

  # PostgreSQL Replica
  postgres-replica:
    image: postgres:14-alpine
    networks:
      - backend
    environment:
      POSTGRES_PASSWORD: secret
      PRIMARY_HOST: postgres-primary
    volumes:
      - postgres-replica:/var/lib/postgresql/data
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.db-role==replica
        max_replicas_per_node: 1

  # PgPool (Connection Pool + LB)
  pgpool:
    image: pgpool/pgpool:latest
    networks:
      - backend
    environment:
      POSTGRES_PASSWORD: secret
      PGPOOL_BACKEND_NODES: "0:postgres-primary:5432,1:postgres-replica:5432"
      PGPOOL_ENABLE_LOAD_BALANCING: "yes"
    deploy:
      replicas: 2
      placement:
        max_replicas_per_node: 1

  # Redis Master
  redis-master:
    image: redis:alpine
    networks:
      - backend
    command: redis-server --appendonly yes
    volumes:
      - redis-master:/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.labels.cache==primary

  # Redis Replica
  redis-replica:
    image: redis:alpine
    networks:
      - backend
    command: redis-server --appendonly yes --slaveof redis-master 6379
    volumes:
      - redis-replica:/data
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.cache==replica
        max_replicas_per_node: 1

  # Redis Sentinel (HA)
  redis-sentinel:
    image: redis:alpine
    networks:
      - backend
    command: redis-sentinel /etc/redis/sentinel.conf
    configs:
      - source: sentinel-config
        target: /etc/redis/sentinel.conf
    deploy:
      replicas: 3
      placement:
        max_replicas_per_node: 1

  # Backup Service
  backup:
    image: postgres:14-alpine
    networks:
      - backend
    volumes:
      - postgres-primary:/source:ro
      - ./backups:/backups
    environment:
      PGPASSWORD: secret
    entrypoint: >
      sh -c "
        while true; do
          pg_dumpall -h postgres-primary -U postgres | gzip > /backups/backup-$(date +%Y%m%d).sql.gz
          find /backups -mtime +7 -delete
          sleep 86400
        done
      "
    deploy:
      replicas: 1

networks:
  frontend:
    driver: overlay
  backend:
    driver: overlay
    internal: true

volumes:
  postgres-primary:
  postgres-replica:
  redis-master:
  redis-replica:

configs:
  nginx-config:
    file: ./nginx.conf
  sentinel-config:
    file: ./sentinel.conf
```

---

## ğŸš« ì•ˆí‹°íŒ¨í„´

### 1. ë‹¨ì¼ Manager

```bash
# âŒ 1ê°œ Manager (SPOF)
docker swarm init
# Manager ì¥ì•  = í´ëŸ¬ìŠ¤í„° ë§ˆë¹„

# âœ… 3ê°œ Manager (HA)
# manager1, manager2, manager3
# 1ëŒ€ ì¥ì•  í—ˆìš©
```

### 2. ë ˆí”Œë¦¬ì¹´ 1ê°œ

```bash
# âŒ ë‹¨ì¼ ë ˆí”Œë¦¬ì¹´
docker service create --replicas 1 myapp
# ë…¸ë“œ ì¥ì•  = ì„œë¹„ìŠ¤ ì¤‘ë‹¨

# âœ… ë‹¤ì¤‘ ë ˆí”Œë¦¬ì¹´
docker service create --replicas 3 myapp
# ë…¸ë“œ ì¥ì•  ì‹œ ë‚˜ë¨¸ì§€ê°€ ì²˜ë¦¬
```

### 3. ë°±ì—… ì—†ìŒ

```bash
# âŒ ë°±ì—… ì „ëµ ì—†ìŒ
# ë°ì´í„° ì†ì‹¤ ì‹œ ë³µêµ¬ ë¶ˆê°€

# âœ… ìë™ ë°±ì—…
# - Daily full backup
# - Hourly incremental
# - 30ì¼ retention
# - DR site sync
```

---

## ğŸ“ í•µì‹¬ ì •ë¦¬

### 1. HA êµ¬ì„± ìš”ì†Œ

```
Manager:
- í™€ìˆ˜ ê°œ (3, 5, 7)
- Raft í•©ì˜
- ì¿¼ëŸ¼ ìœ ì§€

Application:
- ë‹¤ì¤‘ ë ˆí”Œë¦¬ì¹´
- Zone ë¶„ì‚°
- ìë™ ë³µêµ¬

Database:
- Primary-Replica
- Auto Failover
- ë°±ì—…/ë³µêµ¬
```

### 2. ê°€ìš©ì„± ê³„ì‚°

```
SLA ëª©í‘œ:
- 99.9%: ì—° 8.76ì‹œê°„
- 99.99%: ì—° 52.6ë¶„
- 99.999%: ì—° 5.26ë¶„

ë‹¬ì„± ë°©ë²•:
- ì¤‘ë³µì„± (Redundancy)
- ì¥ì•  í—ˆìš© (Fault Tolerance)
- ë¹ ë¥¸ ë³µêµ¬ (MTTR ìµœì†Œí™”)
```

### 3. í•µì‹¬ ëª…ë ¹ì–´

```bash
# Manager
docker node promote <node>
docker node demote <node>

# ë°°ì¹˜
--placement-pref spread=node.labels.<key>
--placement-max-replicas-per-node <N>

# ì¬ì‹œì‘
--restart-condition on-failure
--restart-max-attempts <N>
```

### 4. Best Practices

```
âœ… 3+ Manager ë…¸ë“œ
âœ… Zoneë³„ ë¶„ì‚° ë°°ì¹˜
âœ… ìë™ ë³µêµ¬ ì„¤ì •
âœ… í—¬ìŠ¤ì²´í¬ í•„ìˆ˜
âœ… ì •ê¸° ë°±ì—…
âœ… DR ê³„íš
âœ… ëª¨ë‹ˆí„°ë§/ì•Œë¦¼
```

---

<div align="center">

**[â¬…ï¸ ì´ì „: Rolling Updates](./06-Rolling-Updates.md)** | **[í™ˆìœ¼ë¡œ ğŸ ](../README.md)**

</div>
