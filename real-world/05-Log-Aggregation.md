# 05. Log Aggregation - ELK/EFK Ïä§ÌÉù Íµ¨Ï∂ï

## üéØ Ïù¥ Ï±ïÌÑ∞ÏóêÏÑú Î∞∞Ïö∏ Í≤É

- **Elasticsearch**: Î°úÍ∑∏ Ï†ÄÏû• Î∞è Í≤ÄÏÉâ
- **Logstash**: Î°úÍ∑∏ ÏàòÏßë Î∞è Î≥ÄÌôò
- **Kibana**: Î°úÍ∑∏ ÏãúÍ∞ÅÌôî
- **Fluentd**: Í≤ΩÎüâ Î°úÍ∑∏ ÏàòÏßë
- **Log Î∂ÑÏÑù**: Ïã§Ï†Ñ Î°úÍ∑∏ Î∂ÑÏÑù Í∏∞Î≤ï
- **Ïã§Ï†Ñ Íµ¨ÏÑ±**: Production-ready Î°úÍπÖ

## üìå Ïôú Ï§ëÏöîÌïúÍ∞Ä?

**"Î°úÍ∑∏Îäî Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖòÏùò Î∏îÎûôÎ∞ïÏä§Ïù¥Î©∞, Ï§ëÏïô ÏßëÏ§ëÏãù Î°úÍπÖÏùÄ ÌïÑÏàòÏûÖÎãàÎã§."**

```
Log AggregationÏùò ÌïÑÏöîÏÑ±:

Without Centralized Logging:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Î¨∏Ï†ú Î∞úÏÉù                                         ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ "ÏóêÎü¨Í∞Ä ÎÇ¨ÎäîÎç∞ Ïñ¥ÎîîÏÑú?"                              ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ Container 1: docker logs container1             ‚îÇ
‚îÇ Container 2: docker logs container2             ‚îÇ
‚îÇ Container 3: docker logs container3             ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ ÏàòÎèôÏúºÎ°ú grep, ÏãúÍ∞Ñ ÎßûÏ∂îÍ∏∞ (30Î∂Ñ)                     ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ Container Ïû¨ÏãúÏûë ‚Üí Î°úÍ∑∏ ÏÜêÏã§ ‚ùå                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

With Centralized Logging (ELK):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Î¨∏Ï†ú Î∞úÏÉù                                         ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ KibanaÏóêÏÑú Í≤ÄÏÉâ: "error AND user_id:123"          ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ Î™®Îì† Ïª®ÌÖåÏù¥ÎÑà Î°úÍ∑∏ ÌïúÎààÏóê (5Ï¥à)                        ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ ÏãúÍ∞ÑÏàú Ï†ïÎ†¨, ÌïÑÌÑ∞ÎßÅ, ÏãúÍ∞ÅÌôî                           ‚îÇ
‚îÇ   ‚Üì                                             ‚îÇ
‚îÇ Container Ïû¨ÏãúÏûëÌï¥ÎèÑ Î°úÍ∑∏ Î≥¥Ï°¥ ‚úÖ                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

ELK Stack Architecture:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇContainer1‚îÇ  ‚îÇContainer2‚îÇ  ‚îÇContainer3‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ       ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò             ‚îÇ
‚îÇ                 ‚ñº                               ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ         ‚îÇ   Logstash    ‚îÇ ‚Üê Î°úÍ∑∏ ÏàòÏßë/Î≥ÄÌôò         ‚îÇ
‚îÇ         ‚îÇ   or Fluentd  ‚îÇ                       ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                 ‚ñº                               ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ         ‚îÇ Elasticsearch ‚îÇ ‚Üê Î°úÍ∑∏ Ï†ÄÏû•/Í≤ÄÏÉâ         ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                 ‚ñº                               ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ         ‚îÇ    Kibana     ‚îÇ ‚Üê Î°úÍ∑∏ ÏãúÍ∞ÅÌôî            ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  Features:                                      ‚îÇ
‚îÇ  - Ï§ëÏïô ÏßëÏ§ëÏãù Î°úÍ∑∏                                 ‚îÇ
‚îÇ  - Ïã§ÏãúÍ∞Ñ Í≤ÄÏÉâ                                     ‚îÇ
‚îÇ  - ÏãúÍ∞ÅÌôî ÎåÄÏãúÎ≥¥Îìú                                  ‚îÇ
‚îÇ  - Alert ÏÑ§Ï†ï                                    ‚îÇ
‚îÇ  - Ïû•Í∏∞ Î≥¥Í¥Ä                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

ELK vs EFK:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Component      ‚îÇ ELK        ‚îÇ EFK        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ÏàòÏßë            ‚îÇ Logstash   ‚îÇ Fluentd    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Ï†ÄÏû•/Í≤ÄÏÉâ        ‚îÇ Elasticsearch (Í≥µÌÜµ)     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ÏãúÍ∞ÅÌôî           ‚îÇ Kibana (Í≥µÌÜµ)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Î¶¨ÏÜåÏä§           ‚îÇ ÎÜíÏùå        ‚îÇ ÎÇÆÏùå        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Î≥µÏû°ÎèÑ           ‚îÇ ÎÜíÏùå        ‚îÇ ÎÇÆÏùå        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ÌîåÎü¨Í∑∏Ïù∏         ‚îÇ ÎßéÏùå        ‚îÇ ÎßéÏùå        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Log Flow:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Application                                     ‚îÇ
‚îÇ   ‚Üì (stdout/stderr)                             ‚îÇ
‚îÇ Docker Log Driver                               ‚îÇ
‚îÇ   ‚Üì (json-file, fluentd, gelf)                  ‚îÇ
‚îÇ Logstash/Fluentd                                ‚îÇ
‚îÇ   ‚Üì (parse, filter, enrich)                     ‚îÇ
‚îÇ Elasticsearch                                   ‚îÇ
‚îÇ   ‚Üì (index, search)                             ‚îÇ
‚îÇ Kibana                                          ‚îÇ
‚îÇ   ‚Üì (visualize, alert)                          ‚îÇ
‚îÇ User                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Ïã§Î¨¥ ÏòÅÌñ•:**
- **Îπ†Î•∏ ÎîîÎ≤ÑÍπÖ**: Î∂ÑÏÇ∞ Î°úÍ∑∏ ÌïúÎààÏóê
- **Îç∞Ïù¥ÌÑ∞ Î≥¥Ï°¥**: Ïû¨ÏãúÏûëÌï¥ÎèÑ Ïú†ÏßÄ
- **Ïã§ÏãúÍ∞Ñ Î∂ÑÏÑù**: Ìå®ÌÑ¥ Î∞úÍ≤¨
- **ÏïåÎ¶º**: ÏóêÎü¨ Ï¶âÏãú Í∞êÏßÄ

---

## üîß Ïã§Ïäµ 1: Í∏∞Î≥∏ ELK Stack

### Step 1: Elasticsearch ÏÑ§Ï†ï

```yaml
# docker-compose.elk.yml
version: '3.8'

services:
  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    restart: always
    environment:
      - node.name=elasticsearch
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    networks:
      - elk

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    restart: always
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    ports:
      - "5000:5000/tcp"
      - "5000:5000/udp"
      - "9600:9600"
    environment:
      - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
    depends_on:
      - elasticsearch
    networks:
      - elk

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    restart: always
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=kibana
    depends_on:
      - elasticsearch
    networks:
      - elk

volumes:
  elasticsearch_data:

networks:
  elk:
    driver: bridge
```

### Step 2: Logstash ÏÑ§Ï†ï

```yaml
# logstash/config/logstash.yml
http.host: "0.0.0.0"
xpack.monitoring.elasticsearch.hosts: [ "http://elasticsearch:9200" ]
```

```ruby
# logstash/pipeline/logstash.conf
input {
  # TCP ÏûÖÎ†• (JSON)
  tcp {
    port => 5000
    codec => json
  }

  # UDP ÏûÖÎ†•
  udp {
    port => 5000
    codec => json
  }

  # Beats ÏûÖÎ†•
  beats {
    port => 5044
  }
}

filter {
  # JSON ÌååÏã±
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }

  # ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ ÌååÏã±
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }

  # User Agent ÌååÏã±
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }

  # Grok Ìå®ÌÑ¥ (Nginx Î°úÍ∑∏)
  if [type] == "nginx" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
  }

  # ÌïÑÎìú Ï†úÍ±∞
  mutate {
    remove_field => [ "host", "port" ]
  }
}

output {
  # ElasticsearchÎ°ú Ï†ÑÏÜ°
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }

  # ÎîîÎ≤ÑÍπÖ (stdout)
  stdout {
    codec => rubydebug
  }
}
```

### Step 3: Ïã§Ìñâ Î∞è ÌôïÏù∏

```bash
# Ïã§Ìñâ
docker-compose -f docker-compose.elk.yml up -d

# ÏÉÅÌÉú ÌôïÏù∏
docker-compose -f docker-compose.elk.yml ps

# Elasticsearch ÌôïÏù∏
curl http://localhost:9200
curl http://localhost:9200/_cat/indices?v

# Kibana Ï†ëÏÜç
# http://localhost:5601

# ÌÖåÏä§Ìä∏ Î°úÍ∑∏ Ï†ÑÏÜ°
echo '{"level":"info","message":"Test log","timestamp":"2024-01-15T10:00:00Z"}' | nc localhost 5000
```

---

## üîß Ïã§Ïäµ 2: Application Î°úÍ∑∏ Ï†ÑÏÜ°

### Step 1: Node.js Winston + Logstash

```bash
cd backend
npm install winston winston-logstash
```

```javascript
// backend/logger.js
const winston = require('winston');
const LogstashTransport = require('winston-logstash/lib/winston-logstash-latest');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: {
    service: 'backend',
    environment: process.env.NODE_ENV || 'development'
  },
  transports: [
    // Console (Í∞úÎ∞ú)
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),

    // Logstash (ÌîÑÎ°úÎçïÏÖò)
    new LogstashTransport({
      port: 5000,
      host: 'logstash',
      node_name: 'backend',
      max_connect_retries: -1
    })
  ]
});

module.exports = logger;
```

```javascript
// backend/server.js
const express = require('express');
const logger = require('./logger');

const app = express();

// Request logging middleware
app.use((req, res, next) => {
  const start = Date.now();

  res.on('finish', () => {
    const duration = Date.now() - start;

    logger.info('HTTP Request', {
      method: req.method,
      path: req.path,
      status: res.statusCode,
      duration: duration,
      user_agent: req.get('user-agent'),
      ip: req.ip
    });
  });

  next();
});

// API endpoints
app.get('/api/users', async (req, res) => {
  try {
    logger.info('Fetching users');
    const users = await db.query('SELECT * FROM users');
    res.json(users);
  } catch (err) {
    logger.error('Failed to fetch users', {
      error: err.message,
      stack: err.stack
    });
    res.status(500).json({ error: 'Internal server error' });
  }
});

app.post('/api/users', async (req, res) => {
  try {
    const { username, email } = req.body;

    logger.info('Creating user', { username, email });

    const user = await db.query(
      'INSERT INTO users (username, email) VALUES ($1, $2) RETURNING *',
      [username, email]
    );

    logger.info('User created', { user_id: user.id });

    res.status(201).json(user);
  } catch (err) {
    logger.error('Failed to create user', {
      error: err.message,
      body: req.body
    });
    res.status(500).json({ error: 'Failed to create user' });
  }
});

// Uncaught exceptions
process.on('uncaughtException', (err) => {
  logger.error('Uncaught Exception', {
    error: err.message,
    stack: err.stack
  });
  process.exit(1);
});

app.listen(8080, () => {
  logger.info('Server started', { port: 8080 });
});
```

### Step 2: Docker ComposeÏóê Backend Ï∂îÍ∞Ä

```yaml
# docker-compose.elk.yml
services:
  # ... (ELK Stack)

  # Backend Application
  backend:
    build: ./backend
    container_name: backend
    restart: always
    ports:
      - "8080:8080"
    environment:
      - NODE_ENV=production
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    depends_on:
      - logstash
    networks:
      - elk
```

---

## üîß Ïã§Ïäµ 3: Fluentd (EFK Stack)

### Step 1: Fluentd ÏÑ§Ï†ï

```yaml
# docker-compose.efk.yml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - efk

  # Fluentd
  fluentd:
    build: ./fluentd
    container_name: fluentd
    restart: always
    volumes:
      - ./fluentd/conf:/fluentd/etc
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    ports:
      - "24224:24224"
      - "24224:24224/udp"
    depends_on:
      - elasticsearch
    networks:
      - efk

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - efk

volumes:
  elasticsearch_data:

networks:
  efk:
```

```dockerfile
# fluentd/Dockerfile
FROM fluent/fluentd:v1.16-1

USER root

# Elasticsearch plugin ÏÑ§Ïπò
RUN gem install fluent-plugin-elasticsearch

USER fluent
```

```xml
# fluentd/conf/fluent.conf
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# Docker logs
<source>
  @type tail
  path /var/lib/docker/containers/*/*.log
  pos_file /fluentd/log/containers.log.pos
  tag docker.*
  format json
  time_key time
  time_format %Y-%m-%dT%H:%M:%S.%NZ
</source>

# Parser
<filter docker.**>
  @type parser
  key_name log
  <parse>
    @type json
    time_key timestamp
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</filter>

# Add hostname
<filter docker.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
  </record>
</filter>

# Elasticsearch output
<match docker.**>
  @type elasticsearch
  host elasticsearch
  port 9200
  logstash_format true
  logstash_prefix fluentd
  logstash_dateformat %Y.%m.%d
  include_tag_key true
  tag_key @log_name
  flush_interval 1s
  
  <buffer>
    @type file
    path /fluentd/log/buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_interval 5s
    retry_forever
    retry_max_interval 30
  </buffer>
</match>

# Stdout (debugging)
<match **>
  @type stdout
</match>
```

### Step 2: ApplicationÏóêÏÑú Fluentd ÏÇ¨Ïö©

```yaml
# docker-compose.efk.yml
services:
  backend:
    build: ./backend
    container_name: backend
    restart: always
    logging:
      driver: fluentd
      options:
        fluentd-address: localhost:24224
        tag: docker.backend
    ports:
      - "8080:8080"
    depends_on:
      - fluentd
    networks:
      - efk
```

---

## üîß Ïã§Ïäµ 4: Kibana ÎåÄÏãúÎ≥¥Îìú ÏÑ§Ï†ï

### Step 1: Index Pattern ÏÉùÏÑ±

```bash
# Kibana Ï†ëÏÜç
# http://localhost:5601

# 1. Management ‚Üí Index Patterns
# 2. Create index pattern
# 3. Index pattern: logs-*
# 4. Time field: @timestamp
# 5. Create

# 2. DiscoverÏóêÏÑú Î°úÍ∑∏ ÌôïÏù∏
# Discover ‚Üí Select index pattern: logs-*
```

### Step 2: Í≤ÄÏÉâ ÏøºÎ¶¨ (KQL)

```
# Í∏∞Î≥∏ Í≤ÄÏÉâ
level: "error"

# AND Ï°∞Í±¥
level: "error" AND service: "backend"

# OR Ï°∞Í±¥
level: "error" OR level: "warn"

# Î≤îÏúÑ
status_code >= 400 AND status_code < 500

# Ï°¥Ïû¨ Ïó¨Î∂Ä
user_id: *

# Ï†ïÍ∑ú ÌëúÌòÑÏãù
path: /api/users/*

# ÏãúÍ∞Ñ Î≤îÏúÑ
@timestamp >= "2024-01-15" AND @timestamp <= "2024-01-16"

# Î≥µÌï© ÏøºÎ¶¨
level: "error" AND service: "backend" AND NOT path: "/health"
```

### Step 3: Visualization ÏÉùÏÑ±

```bash
# 1. Visualize ‚Üí Create visualization
# 2. Ï∞®Ìä∏ ÌÉÄÏûÖ ÏÑ†ÌÉù:
#    - Line chart: ÏãúÍ∞ÑÎ≥Ñ Ï∂îÏù¥
#    - Bar chart: ÎπÑÍµê
#    - Pie chart: Î∂ÑÌè¨
#    - Metric: Îã®Ïùº Í∞í

# Ïòà: Error Rate Over Time
# - X-axis: Date Histogram (@timestamp)
# - Y-axis: Count
# - Filter: level: "error"

# Ïòà: Top 10 Error Messages
# - Chart type: Bar
# - X-axis: Terms (message.keyword)
# - Y-axis: Count
# - Size: 10
```

### Step 4: Dashboard ÏÉùÏÑ±

```bash
# 1. Dashboard ‚Üí Create dashboard
# 2. Add panels (Visualizations)
# 3. Arrange and save

# Ï∂îÏ≤ú Dashboard Panels:
# - Total Requests (Metric)
# - Requests Over Time (Line)
# - Error Rate (Line)
# - Top Endpoints (Bar)
# - Status Code Distribution (Pie)
# - Response Time (Line)
```

---

## üîß Ïã§Ïäµ 5: Log Alerting

### Step 1: Watcher ÏÑ§Ï†ï (Elasticsearch)

```json
// kibana/watcher/high-error-rate.json
{
  "trigger": {
    "schedule": {
      "interval": "5m"
    }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["logs-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                {
                  "range": {
                    "@timestamp": {
                      "gte": "now-5m"
                    }
                  }
                },
                {
                  "term": {
                    "level": "error"
                  }
                }
              ]
            }
          },
          "aggs": {
            "error_count": {
              "value_count": {
                "field": "@timestamp"
              }
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": {
      "ctx.payload.aggregations.error_count.value": {
        "gt": 10
      }
    }
  },
  "actions": {
    "send_email": {
      "email": {
        "to": "admin@example.com",
        "subject": "High Error Rate Detected",
        "body": "Error count in last 5 minutes: {{ctx.payload.aggregations.error_count.value}}"
      }
    },
    "send_slack": {
      "webhook": {
        "scheme": "https",
        "host": "hooks.slack.com",
        "port": 443,
        "method": "post",
        "path": "/services/YOUR/WEBHOOK/URL",
        "params": {},
        "headers": {
          "Content-Type": "application/json"
        },
        "body": "{\"text\": \"‚ö†Ô∏è High Error Rate: {{ctx.payload.aggregations.error_count.value}} errors in 5 minutes\"}"
      }
    }
  }
}
```

### Step 2: Elastalert (ÎåÄÏïà)

```yaml
# elastalert/config.yaml
rules_folder: rules
run_every:
  minutes: 1
buffer_time:
  minutes: 15
es_host: elasticsearch
es_port: 9200
writeback_index: elastalert_status
alert_time_limit:
  days: 2
```

```yaml
# elastalert/rules/high_error_rate.yaml
name: High Error Rate
type: frequency
index: logs-*
num_events: 10
timeframe:
  minutes: 5

filter:
- term:
    level: "error"

alert:
- "email"
- "slack"

email:
- "admin@example.com"

slack_webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
slack_username_override: "ElastAlert"
slack_emoji_override: ":warning:"
```

---

## üîß Ïã§Ïäµ 6: ÏôÑÏ†ÑÌïú ELK Stack with Application

### Step 1: Ï†ÑÏ≤¥ Docker Compose

```yaml
# docker-compose.production.yml
version: '3.8'

services:
  # Application
  backend:
    build: ./backend
    container_name: backend
    restart: always
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    ports:
      - "8080:8080"
    environment:
      - NODE_ENV=production
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    depends_on:
      - postgres
      - logstash
    networks:
      - app
      - elk

  postgres:
    image: postgres:15-alpine
    container_name: postgres
    restart: always
    environment:
      POSTGRES_DB: mydb
      POSTGRES_USER: myuser
      POSTGRES_PASSWORD: mypassword
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - app

  # ELK Stack
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    restart: always
    environment:
      - node.name=elasticsearch
      - cluster.name=docker-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
      - xpack.security.enabled=false
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
    networks:
      - elk

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: logstash
    restart: always
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
    ports:
      - "5000:5000/tcp"
      - "5000:5000/udp"
    environment:
      - "LS_JAVA_OPTS=-Xms512m -Xmx512m"
    depends_on:
      - elasticsearch
    networks:
      - elk

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    restart: always
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    networks:
      - elk

  # Nginx (Reverse Proxy)
  nginx:
    image: nginx:alpine
    container_name: nginx
    restart: always
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - backend
      - kibana
    networks:
      - app
      - elk

volumes:
  postgres_data:
  elasticsearch_data:

networks:
  app:
    driver: bridge
  elk:
    driver: bridge
```

### Step 2: Nginx ÏÑ§Ï†ï (Kibana ÌîÑÎ°ùÏãú)

```nginx
# nginx/nginx.conf
server {
    listen 80;
    server_name localhost;

    # Application
    location / {
        proxy_pass http://backend:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # Kibana
    location /kibana/ {
        proxy_pass http://kibana:5601/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Basic Auth (ÏÑ†ÌÉù)
        auth_basic "Kibana";
        auth_basic_user_file /etc/nginx/.htpasswd;
    }
}
```

---

## üí° Ï£ºÏöî Ìå®ÌÑ¥ Ï†ïÎ¶¨

```
Log Levels:
- TRACE: Îß§Ïö∞ ÏÉÅÏÑ∏Ìïú ÎîîÎ≤ÑÍπÖ
- DEBUG: ÎîîÎ≤ÑÍπÖ Ï†ïÎ≥¥
- INFO: ÏùºÎ∞ò Ï†ïÎ≥¥ (Í∏∞Î≥∏)
- WARN: Í≤ΩÍ≥†
- ERROR: ÏóêÎü¨
- FATAL: ÏπòÎ™ÖÏ†Å ÏóêÎü¨

Structured Logging (JSON):
{
  "timestamp": "2024-01-15T10:00:00Z",
  "level": "INFO",
  "service": "backend",
  "message": "User created",
  "user_id": 123,
  "ip": "192.168.1.1",
  "duration": 45
}

Log Retention:
- Hot (Í≤ÄÏÉâ): 7Ïùº
- Warm (Î≥¥Í¥Ä): 30Ïùº
- Cold (ÏïÑÏπ¥Ïù¥Î∏å): 1ÎÖÑ+

Index Management:
- Daily: logs-2024.01.15
- Monthly: logs-2024.01
- ILM (Index Lifecycle Management)
```

---

## üìå ÌïµÏã¨ ÏöîÏïΩ

```
Log Aggregation ÌïµÏã¨:
1. Ï§ëÏïô ÏßëÏ§ëÏãù Î°úÍ∑∏
2. Structured Logging (JSON)
3. Ïã§ÏãúÍ∞Ñ Í≤ÄÏÉâ (Elasticsearch)
4. ÏãúÍ∞ÅÌôî (Kibana)
5. Alert ÏÑ§Ï†ï

ELK vs EFK:
- ELK: Logstash (Î¨¥Í≤ÅÏßÄÎßå Í∞ïÎ†•)
- EFK: Fluentd (Í∞ÄÎ≥çÍ≥† Ïú†Ïó∞)

Best Practices:
‚úÖ JSON Î°úÍ∑∏ Ìè¨Îß∑
‚úÖ Ï†ÅÏ†àÌïú Log Level
‚úÖ Context Ï†ïÎ≥¥ Ìè¨Ìï®
‚úÖ ÎØºÍ∞ê Ï†ïÎ≥¥ Ï†úÏô∏
‚úÖ Index Lifecycle Í¥ÄÎ¶¨
```

---

## üìö Ï∞∏Í≥† ÏûêÎ£å

- [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)
- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
- [Kibana Documentation](https://www.elastic.co/guide/en/kibana/current/index.html)
- [Fluentd Documentation](https://docs.fluentd.org/)
- [Structured Logging Best Practices](https://www.datadoghq.com/blog/log-file-formats/)

---

## ü§î ÏÉùÍ∞ÅÌï¥Î≥º Î¨∏Ï†ú

1. ELK StackÍ≥º EFK Stack Ï§ë Ïñ¥Îäê Í≤ÉÏùÑ ÏÑ†ÌÉùÌï¥Ïïº ÌïòÎäîÍ∞Ä?
2. Î™®Îì† Î°úÍ∑∏Î•º Ï†ÄÏû•Ìï¥Ïïº ÌïòÎäîÍ∞Ä?
3. ElasticsearchÎäî Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Ïù∏Í∞Ä?

> üí° **ÎãµÎ≥Ä**:
> 
> **1) ELK vs EFK ÏÑ†ÌÉù:**
> 
> ```
> Îëò Îã§ Ïö∞Ïàò, ÏÉÅÌô©Ïóê Îî∞Îùº ÏÑ†ÌÉù
> 
> ELK (Elasticsearch + Logstash + Kibana):
> ‚úÖ Í∞ïÎ†•Ìïú Îç∞Ïù¥ÌÑ∞ Î≥ÄÌôò (Logstash)
> ‚úÖ Î≥µÏû°Ìïú ÌååÏã±
> ‚úÖ Îã§ÏñëÌïú Input Plugin
> ‚úÖ ÏÑ±ÏàôÌïú ÏÉùÌÉúÍ≥Ñ
> 
> Îã®Ï†ê:
> ‚ùå Î¨¥Í±∞ÏõÄ (Java, JVM)
> ‚ùå ÎÜíÏùÄ Î©îÎ™®Î¶¨ (512MB+)
> ‚ùå Î≥µÏû°Ìïú ÏÑ§Ï†ï
> 
> ÏÇ¨Ïö© ÏÇ¨Î°Ä:
> - Î≥µÏû°Ìïú Î°úÍ∑∏ ÌååÏã±
> - Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§
> - Í≥†Í∏â Î≥ÄÌôò ÌïÑÏöî
> - Ï∂©Î∂ÑÌïú Î¶¨ÏÜåÏä§
> 
> EFK (Elasticsearch + Fluentd + Kibana):
> ‚úÖ Í∞ÄÎ≤ºÏõÄ (Ruby, C)
> ‚úÖ Ï†ÅÏùÄ Î©îÎ™®Î¶¨ (40MB)
> ‚úÖ Í∞ÑÎã®Ìïú ÏÑ§Ï†ï
> ‚úÖ Ïø†Î≤ÑÎÑ§Ìã∞Ïä§ ÌëúÏ§Ä
> 
> Îã®Ï†ê:
> ‚ùå Ï†úÌïúÏ†Å Î≥ÄÌôò
> ‚ùå Ï†ÅÏùÄ ÌîåÎü¨Í∑∏Ïù∏
> ‚ùå Îü¨Îãù Ïª§Î∏å (Îã§Î•∏ Î¨∏Î≤ï)
> 
> ÏÇ¨Ïö© ÏÇ¨Î°Ä:
> - Ïø†Î≤ÑÎÑ§Ìã∞Ïä§
> - Ïª®ÌÖåÏù¥ÎÑà ÌôòÍ≤Ω
> - Î¶¨ÏÜåÏä§ Ï†úÌïú
> - Îã®Ïàú Î°úÍ∑∏ ÏàòÏßë
> 
> ÏÑ±Îä• ÎπÑÍµê:
> Logstash: 512MB RAM, 20K events/sec
> Fluentd:   40MB RAM, 15K events/sec
> 
> Ïã§Ï†Ñ ÏÑ†ÌÉù:
> 
> Kubernetes ‚Üí Fluentd
> - CNCF ÌîÑÎ°úÏ†ùÌä∏
> - DaemonSet ÏµúÏ†ÅÌôî
> - Ï†ÅÏùÄ Ïò§Î≤ÑÌó§Îìú
> 
> Î≥µÏû°Ìïú ÌååÏã± ‚Üí Logstash
> - Grok Ìå®ÌÑ¥ (Í∞ïÎ†•)
> - ÌíçÎ∂ÄÌïú ÌïÑÌÑ∞
> - Îã§ÏñëÌïú ÏûÖÎ†•
> 
> Docker Swarm ‚Üí Fluentd
> - Docker Î°úÍ∑∏ ÎìúÎùºÏù¥Î≤Ñ ÎÑ§Ïù¥Ìã∞Î∏å
> - Í∞ÄÎ≤ºÏõÄ
> 
> Í≤∞Î°†:
> Ïø†Î≤ÑÎÑ§Ìã∞Ïä§/Í∞ÑÎã® ‚Üí Fluentd
> Î≥µÏû°Ìïú Î≥ÄÌôò ‚Üí Logstash
> ÏÑ±Îä• Ï∞®Ïù¥ ÌÅ¨ÏßÄ ÏïäÏùå (Îëò Îã§ Ïö∞Ïàò)
> ```
> 
> **2) Î™®Îì† Î°úÍ∑∏ Ï†ÄÏû•?**
> 
> ```
> NO! ÏÑ†ÌÉùÏ†ÅÏúºÎ°ú
> 
> Î¨∏Ï†ú:
> - Ïä§ÌÜ†Î¶¨ÏßÄ Ìè≠Î∞ú (100GB/day)
> - Í≤ÄÏÉâ ÎäêÎ†§Ïßê
> - ÎπÑÏö© Ï¶ùÍ∞Ä
> - ÎÖ∏Ïù¥Ï¶à ÎßéÏùå
> 
> Î°úÍ∑∏ Î†àÎ≤® Ï†ÑÎûµ:
> 
> ÌîÑÎ°úÎçïÏÖò:
> ‚úÖ ERROR (Ï†ÄÏû•, ÏïåÎ¶º)
> ‚úÖ WARN (Ï†ÄÏû•)
> ‚ö†Ô∏è INFO (ÏÑ†ÌÉùÏ†Å)
> ‚ùå DEBUG (Ï†ÄÏû• Ïïà Ìï®)
> ‚ùå TRACE (Ï†ÄÏû• Ïïà Ìï®)
> 
> Í∞úÎ∞ú:
> ‚úÖ Î™®Îì† Î†àÎ≤®
> 
> ÏÉòÌîåÎßÅ:
> # ÏÑ±Í≥µ ÏöîÏ≤≠ 10%Îßå Ï†ÄÏû•
> if status_code == 200:
>   if random() < 0.1:
>     log()
> 
> # ÏóêÎü¨Îäî 100% Ï†ÄÏû•
> if status_code >= 500:
>   log()
> 
> ÌïÑÌÑ∞ÎßÅ:
> # Health check Î°úÍ∑∏ Ï†úÏô∏
> filter {
>   if [path] == "/health" {
>     drop { }
>   }
> }
> 
> Î≥¥Ï°¥ Í∏∞Í∞Ñ:
> - Hot (Îπ†Î•∏ Í≤ÄÏÉâ): 7Ïùº
> - Warm (Î≥¥Í¥Ä): 30Ïùº
> - Cold (ÏïÑÏπ¥Ïù¥Î∏å): 90Ïùº
> - ÏÇ≠Ï†ú: 90Ïùº Ïù¥ÏÉÅ
> 
> Index Lifecycle:
> PUT _ilm/policy/logs_policy
> {
>   "policy": {
>     "phases": {
>       "hot": {
>         "actions": {}
>       },
>       "warm": {
>         "min_age": "7d",
>         "actions": {
>           "forcemerge": { "max_num_segments": 1 }
>         }
>       },
>       "cold": {
>         "min_age": "30d",
>         "actions": {
>           "freeze": {}
>         }
>       },
>       "delete": {
>         "min_age": "90d",
>         "actions": {
>           "delete": {}
>         }
>       }
>     }
>   }
> }
> 
> ÎπÑÏö© Í≥ÑÏÇ∞:
> 100GB/day √ó 30Ïùº = 3TB
> AWS EBS: $300/month
> 
> 10GB/day √ó 30Ïùº = 300GB
> AWS EBS: $30/month
> 
> ‚Üí 10Î∞∞ Ï†àÍ∞ê!
> 
> Í≤∞Î°†:
> ERROR/WARN ‚Üí 100% Ï†ÄÏû•
> INFO ‚Üí ÏÉòÌîåÎßÅ (10-20%)
> DEBUG/TRACE ‚Üí Í∞úÎ∞úÎßå
> Health check ‚Üí Ï†úÏô∏
> Î≥¥Ï°¥ Í∏∞Í∞Ñ ‚Üí 30-90Ïùº
> ```
> 
> **3) ElasticsearchÎäî DB?**
> 
> ```
> NO! Search Engine
> (ÌïòÏßÄÎßå Ïú†ÏÇ¨Ï†ê ÏûàÏùå)
> 
> Elasticsearch:
> ‚úÖ Full-text Search (Í≤ÄÏÉâ ÏóîÏßÑ)
> ‚úÖ Îπ†Î•∏ Í≤ÄÏÉâ (Ïó≠Ïù∏Îç±Ïä§)
> ‚úÖ Analytics (ÏßëÍ≥Ñ)
> ‚úÖ ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞
> 
> ‚ùå Ï£º Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§Î°ú Î∂ÄÏ†ÅÌï©
> ‚ùå ACID Ìä∏ÎûúÏû≠ÏÖò ÏóÜÏùå
> ‚ùå JOIN Ï†úÌïúÏ†Å
> ‚ùå Îç∞Ïù¥ÌÑ∞ Ï†ïÌï©ÏÑ± ÏïΩÌï®
> ‚ùå ÏóÖÎç∞Ïù¥Ìä∏/ÏÇ≠Ï†ú ÎπÑÏö© ÎÜíÏùå
> 
> vs Ï†ÑÌÜµ DB:
> 
> PostgreSQL:
> - ACID Î≥¥Ïû•
> - Strong Consistency
> - Î≥µÏû°Ìïú ÏøºÎ¶¨ (JOIN)
> - Ìä∏ÎûúÏû≠ÏÖò
> ‚Üí Primary Database
> 
> Elasticsearch:
> - Eventually Consistent
> - Full-text Search
> - Î°úÍ∑∏/Î©îÌä∏Î¶≠
> - ÏùΩÍ∏∞ ÏµúÏ†ÅÌôî
> ‚Üí Search/Analytics
> 
> Ïò¨Î∞îÎ•∏ ÏÇ¨Ïö©:
> 
> ‚úÖ Î°úÍ∑∏ Í≤ÄÏÉâ
> ‚úÖ Î©îÌä∏Î¶≠ Ï†ÄÏû•
> ‚úÖ Ï†ÑÎ¨∏ Í≤ÄÏÉâ
> ‚úÖ Ïã§ÏãúÍ∞Ñ Î∂ÑÏÑù
> ‚úÖ ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞
> 
> ‚ùå ÏÇ¨Ïö©Ïûê Îç∞Ïù¥ÌÑ∞ (DB ÏÇ¨Ïö©)
> ‚ùå Ìä∏ÎûúÏû≠ÏÖò (DB ÏÇ¨Ïö©)
> ‚ùå Í∏àÏúµ Îç∞Ïù¥ÌÑ∞ (DB ÏÇ¨Ïö©)
> ‚ùå Ï†ïÌï©ÏÑ± Ï§ëÏöî (DB ÏÇ¨Ïö©)
> 
> ÏïÑÌÇ§ÌÖçÏ≤ò:
> 
> App
>  ‚Üì
> PostgreSQL (Primary)
>  ‚Üì (Î≥µÏ†ú)
> Elasticsearch (Search)
> 
> ‚Üí DBÏóê Ï†ÄÏû•
> ‚Üí ESÏóê Ïù∏Îç±Ïã±
> ‚Üí ESÏóêÏÑú Í≤ÄÏÉâ
> 
> Ïòà:
> # ÏÇ¨Ïö©Ïûê ÏÉùÏÑ± ‚Üí DB
> INSERT INTO users ...
> 
> # Í≤ÄÏÉâÏö© Ïù∏Îç±Ïã± ‚Üí ES
> POST /users/_doc
> {
>   "name": "John",
>   "email": "john@example.com"
> }
> 
> # Í≤ÄÏÉâ ‚Üí ES
> GET /users/_search
> {
>   "query": {
>     "match": { "name": "John" }
>   }
> }
> 
> # ÏóÖÎç∞Ïù¥Ìä∏ ‚Üí DB
> UPDATE users ...
> 
> Í≤∞Î°†:
> Elasticsearch = Search Engine
> Î°úÍ∑∏/Í≤ÄÏÉâ/Î∂ÑÏÑùÏö©
> Primary DBÎ°ú Î∂ÄÏ†ÅÌï©
> DB + ES Ï°∞Ìï©Ïù¥ Best
> ```


---

<div align="center">

**[‚¨ÖÔ∏è Ïù¥Ï†Ñ: Monitoring Stack](./04-Monitoring-Stack.md)** | **[Îã§Ïùå: Backup System ‚û°Ô∏è](./06-Backup-System.md)**

</div>
